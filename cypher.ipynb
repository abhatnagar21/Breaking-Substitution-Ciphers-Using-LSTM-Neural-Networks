{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "C66_hjEUQt8W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding#lstm is a type of recurrent neural network (RNN) layer that handles sequence and time-series data\n",
        "#dense:A fully connected (dense) layer for the output or intermediate layers in the network.Used to convert input tokens (like words or characters) into dense vector representations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##random substitution cipher\n",
        "def generate_cipher_map():\n",
        "    \"\"\"Generates a random substitution cipher map.\"\"\"\n",
        "    letters=list(string.ascii_lowercase)#english alphabet a-z\n",
        "    shuffled=letters.copy()\n",
        "    random.shuffle(shuffled)#shuffles the shuffled list which is the copy of the original list\n",
        "    cipher_map={plain: cipher for plain, cipher in zip(letters,shuffled)}#map letter->shuffled letter plain becomes key cipher becomes the values\n",
        "    reverse_map={cipher:plain for plain, cipher in cipher_map.items()}#reverse mapping for decryption\n",
        "    return cipher_map,reverse_map"
      ],
      "metadata": {
        "id": "H1LeQthvRKoc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encrypt using cipher map\n",
        "def encrypt_text(text,cipher_map):\n",
        "    return ''.join(cipher_map.get(char,char) for char in text)# checks for char in the map, if yes then maps it to the cipher map else remains as it is"
      ],
      "metadata": {
        "id": "_KXmoUiKRMhG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate dataset plaintext-ciphertext pairs using a random substitution cipher.\n",
        "def generate_dataset(num_samples=10000,max_len=20):\n",
        "    cipher_map,reverse_map=generate_cipher_map()#substitution text and reverse for decryption\n",
        "    plaintexts=[]\n",
        "    ciphertexts=[]\n",
        "    for _ in range(num_samples):#run num sample times\n",
        "        plain_text=''.join(random.choices(string.ascii_lowercase+' ',k=random.randint(5,max_len)))# random selection of lower case letters with spaces ranging from 5 to 20 length\n",
        "        cipher_text=encrypt_text(plain_text,cipher_map)#plain letters to cipher map\n",
        "        plaintexts.append(plain_text)# ADD TO THE LIST\n",
        "        ciphertexts.append(cipher_text)\n",
        "    return plaintexts, ciphertexts, reverse_map"
      ],
      "metadata": {
        "id": "vhC-JvPCRMje"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replaces each character in the text with a corresponding number based on its position in the vocabulary for lstm\n",
        "def tokenize_texts(texts,vocab):#vocab defines the letter to be tokenised\n",
        "    tokenizer={char: idx+1 for idx, char in enumerate(vocab)}#give each char in vocab a number and add 1 to index 0 used for padding\n",
        "    sequences=[[tokenizer[char] for char in text] for text in texts]#each character with its corresponding number from the tokenizer dictionary\n",
        "    return sequences,tokenizer"
      ],
      "metadata": {
        "id": "8u5YeGyDRMlu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(sequences,max_len):\n",
        "    # ensures that all sequences of integers (tokenized text data) have the same length by adding padding for lstm model\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')# padding at last for same length"
      ],
      "metadata": {
        "id": "DRUjZVNGRMnN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_dim,output_dim,max_len):\n",
        "   #builds a sequence-to-sequence model using LSTM layers constructs a sequence-to-sequence model using LSTM layers. It's designed to map input sequences (like tokenized plaintext) to output sequences (like tokenized ciphertext) for tasks like text transformation.\n",
        "    model=Sequential([\n",
        "        Embedding(input_dim=input_dim,output_dim=64,input_length=max_len),\n",
        "        LSTM(128,return_sequences=True),#Outputs a sequence (one prediction per input token)\n",
        "        Dense(output_dim,activation='softmax')#This predicts the probability distribution for each token in the output sequence.\n",
        "    ])\n",
        "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "    return model\n",
        "    #LSTM layer processes sequential data effectively by remembering dependencies over long sequences."
      ],
      "metadata": {
        "id": "4upffMrTRMoW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    print(\"Generating dataset...\")\n",
        "    vocab=list(string.ascii_lowercase + ' ')# a-z and space\n",
        "    num_classes=len(vocab)+1# a-z 26 + space+0 padding = 28\n",
        "    num_samples=10000\n",
        "    max_len=20\n",
        "    plaintexts,ciphertexts,reverse_map=generate_dataset(num_samples, max_len)\n",
        "    print(\"Sample Plaintext:\", plaintexts[0])#print first plain text\n",
        "    print(\"Sample Ciphertext:\", ciphertexts[0])#print first cypher text\n",
        "    #tokenize and pad texts\n",
        "    plaintext_seq,plain_tokenizer=tokenize_texts(plaintexts,vocab)#tokenise plain text to number for lstm\n",
        "    ciphertext_seq,cipher_tokenizer=tokenize_texts(ciphertexts,vocab) #tokenise pypher text to number\n",
        "    X=pad_sequences(ciphertext_seq,max_len)#cyphertex->encryption\n",
        "    y=pad_sequences(plaintext_seq,max_len)#plain text to original text\n",
        "    y=np.expand_dims(y,-1)#dd an extra dimension for sparse categorical loss 2d->3d\n",
        "    #Sparse Categorical Loss: In this type of loss function, y must be a 2D array of shape (num_samples, sequence_length), where each element is an integer representing the target class.\n",
        "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "    print(\"Building model...\")\n",
        "    model=build_model(input_dim=num_classes,output_dim=num_classes,max_len=max_len)\n",
        "    model.summary()\n",
        "    print(\"Training model...\")\n",
        "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32)\n",
        "     # Testing the model\n",
        "    idx_to_char={idx: char for char, idx in plain_tokenizer.items()}#integer index->corresponding text\n",
        "    print(\"\\nTesting model on encrypted text:\")\n",
        "    test_sample=X_test[0]\n",
        "    predicted=model.predict(np.array([test_sample]))\n",
        "    predicted_text=''.join([idx_to_char.get(np.argmax(p), '') for p in predicted[0]])\n",
        "    original_text=''.join([idx_to_char.get(idx, '') for idx in y_test[0].flatten()])\n",
        "    encrypted_text=''.join([idx_to_char.get(idx, '') for idx in test_sample])\n",
        "    print(f\"Encrypted:{encrypted_text}\")\n",
        "    print(f\"Predicted Plaintext:{predicted_text}\")\n",
        "    print(f\"Original Plaintext:{original_text}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    #out of 10,000 data in the set model trains on 80% and tests on 20% of the random data generated\n"
      ],
      "metadata": {
        "id": "yykcZCbXRMqH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "5d61a37f-1b31-4b0c-91c5-6b863a7d0a7a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating dataset...\n",
            "Sample Plaintext: gwoperwlvrahtce\n",
            "Sample Ciphertext: wvijxmvqcmotpsx\n",
            "Building model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Epoch 1/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 40ms/step - accuracy: 0.5522 - loss: 2.0724 - val_accuracy: 1.0000 - val_loss: 0.0267\n",
            "Epoch 2/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0151 - val_accuracy: 1.0000 - val_loss: 0.0035\n",
            "Epoch 3/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
            "Epoch 4/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 7.4711e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 6.6077e-04 - val_accuracy: 1.0000 - val_loss: 4.6621e-04\n",
            "\n",
            "Testing model on encrypted text:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
            "Encrypted:lnhnjrptjhdl d\n",
            "Predicted Plaintext:fdmdpnthpmsf s\n",
            "Original Plaintext:fdmdpnthpmsf s\n"
          ]
        }
      ]
    }
  ]
}